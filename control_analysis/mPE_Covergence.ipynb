{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import itertools as it\n",
    "import scipy.special as psi\n",
    "plt.style.use('classic')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math as mt\n",
    "import time\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import stats\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from scipy.integrate import quad\n",
    "from scipy.io import savemat\n",
    "from tempfile import TemporaryFile\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from mpl_toolkits import mplot3d\n",
    "# from mPE_fn import mPE\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import entropy\n",
    "from mPE_ultis import integrand, ubble, array_list, permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiscale Permutation Entropy (mPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [H,h]=mPE(data,n)\n",
    "# returns the permutation entropy (of order n) and normalised \n",
    "# entropy vector of a time series \"data\"\n",
    "    \n",
    "# [H,h]=mPE(data,n,eps)\n",
    "# returns the permutation entropy (of order n) and normalised \n",
    "# entropy vector of a time series \"data\". The vector eps contains\n",
    "# as many elements as the number of scales considered by the algorithm.\n",
    "# Each element of the vector eps represents how many elements are \n",
    "# considered in creating a coarse-grained time series. Default is eps=[1]\n",
    "    \n",
    "# [H,h]=mPE(data,n,eps,L)\n",
    "# returns the permutation entropy (of order n) and normalised \n",
    "# entropy vector of a time series \"data\". The vector eps contains\n",
    "# as many elements as the number of scales considered by the algorithm.\n",
    "# Each element of the vector eps represents how many elements are \n",
    "# considered in creating a coarse-grained time series. Default is eps=[1]\n",
    "# L is a lag parameter, default is 1.\n",
    "# \n",
    "# n(order of the entropy)\n",
    "# L(distance between two adjacent elements in the computation)(generally=1)\n",
    "\n",
    "def mPE_(*argv):    \n",
    "    g=len(argv)\n",
    "    \n",
    "    if g==2:\n",
    "        n=argv[1]\n",
    "        datain=argv[0]\n",
    "        lamin=np.array([1])\n",
    "        epsin=np.array([1])\n",
    "    if g==3:\n",
    "        n=argv[1]\n",
    "        datain=argv[0]\n",
    "        lamin=np.array([1])\n",
    "        epsin=argv[2]\n",
    "    if g==4:\n",
    "        n=argv[1]\n",
    "        datain=argv[0]\n",
    "        lamin=argv[3]\n",
    "        epsin=argv[2]\n",
    "    \n",
    "#    Check for the data to be in the right dimension\n",
    "    if isinstance(datain, list):\n",
    "        if len(datain[:,1])>len(datain[1,:]):\n",
    "            datain=datain.transpose()\n",
    "    else:\n",
    "        print('check')\n",
    "        if len(datain[:,0])>len(datain[0,:]):\n",
    "            datain=datain.transpose()\n",
    "        \n",
    "    scalesno=len(epsin)\n",
    "    lagno=len(lamin)\n",
    "    HH=np.zeros((lagno,scalesno))\n",
    "    norm_HH=np.zeros((lagno,scalesno))\n",
    "#    Definition of parameters: fac is the number of possible permutations\n",
    "#    Elem is the No of data points\n",
    "#    Dim is the dimensionality of the samples\n",
    "    \n",
    "    for lam in range(0,lagno):\n",
    "        for eps in range(0,scalesno):\n",
    "            scale=epsin[eps]\n",
    "            L=lamin[lam]\n",
    "            xlen=len(datain[0,:])\n",
    "            ylen=np.round((xlen/scale)-0.5)\n",
    "            ylen=ylen.astype(int)\n",
    "            D=np.zeros((len(datain[:,0]),ylen))\n",
    "            for ylenc in range(0,ylen):\n",
    "                dfg=datain[:,((ylenc)*scale):((ylenc+1)*scale)]\n",
    "                fhk=np.sum(dfg,axis=1)\n",
    "                r=(1/scale)*fhk\n",
    "                D[:,ylenc]=r\n",
    "                \n",
    "            data=D\n",
    "            fac=mt.factorial(n)\n",
    "            elem=len(data[0,:])\n",
    "            dim=len(data[:,0])\n",
    "            \n",
    "        #    A is a n-by-factorial(n) matrix that shows all the \n",
    "        #    possible permutations of n elements\n",
    "            A=permutation(n)\n",
    "            \n",
    "        #    counter is a factorial(n) square matrix that counts the recurrence of\n",
    "        #    a dim-dimensional permutation\n",
    "            lpi=fac*np.ones((dim))\n",
    "            lpi=lpi.astype(int)\n",
    "            nj = lpi.tolist()\n",
    "            counter=np.zeros((nj))\n",
    "            \n",
    "        #    For each iteration i, a series of n points is sampled from the \n",
    "        #    data set and the corresponding permutation is identified. \n",
    "        #    The counter matrix keeps track of the numiber of times a certain\n",
    "        #    permutation, or combination of permutations is observed. \n",
    "            for i in range(0, elem-n*L+L-1):\n",
    "                coord=np.zeros((dim))\n",
    "                for num in range(0,dim):\n",
    "                    sample=data[num,range(i,i+L*(n),L)]\n",
    "                    ord=ubble(sample)\n",
    "                    perm_num=0\n",
    "                    check_1=1\n",
    "                    check_2=1\n",
    "                    \n",
    "                    while (perm_num<=fac) and (check_2):\n",
    "                       \n",
    "                        check_1=1\n",
    "                        for j in range(0,n-1):\n",
    "                            if ord[j]!=A[perm_num,j]:\n",
    "                                check_1=0\n",
    "                        if check_1:\n",
    "                            coord[num]=perm_num\n",
    "                            check_2=0\n",
    "                            \n",
    "                        perm_num=perm_num+1\n",
    "                \n",
    "                    \n",
    "                coord=coord.astype(int)\n",
    "                coord1=tuple(coord)\n",
    "                counter[coord1]=counter[coord1]+1\n",
    "                \n",
    "        #    Once the counter matrix is complete, each element is divided by the\n",
    "        #    total number of samples to get a empirical probability, and the PE is\n",
    "        #    computed according to the method described in Schurmann\n",
    "            \n",
    "            H=0\n",
    "            counter1=counter.flatten()\n",
    "            for iter in range(0,(fac**dim)):\n",
    "               \n",
    "                rec=counter1[iter]\n",
    "                tot=elem-n+1\n",
    "                \n",
    "                if rec==0:\n",
    "                    H=H\n",
    "                    \n",
    "                else:\n",
    "                    I = quad(integrand, 0, 1, args=(rec))\n",
    "                    I=I[0]\n",
    "                    coeff=((-1)**rec)*I\n",
    "                    prob=(rec/tot)*(psi.digamma(tot)-psi.digamma(rec)-coeff)\n",
    "                    H=H+mt.log2(mt.exp(prob))\n",
    "                 \n",
    "        #     The normalised entropy norm_H is computed as well\n",
    "            norm_H=H/(mt.log2(fac**dim))\n",
    "            \n",
    "            HH[lam,eps]=H\n",
    "            norm_HH[lam,eps]=norm_H\n",
    "    return [HH,norm_HH]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis - Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "orders = [3, 4]\n",
    "lengths = [6, 8, 10]\n",
    "sizes = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000]\n",
    "bound_len = 1e07\n",
    "n_trials = 20\n",
    "# convergence analysis over 20 trials, 3 dims and different lengths\n",
    "for n_PC in range(2,4):\n",
    "    for i, order in enumerate(orders):\n",
    "        for length_ in lengths:\n",
    "            # initialize sample entropy array\n",
    "            sample_H = np.zeros((n_trials, len(sizes)))\n",
    "            for trial in range(n_trials):\n",
    "                # create fundamental unit for synthetic data\n",
    "                f_unit = np.random.rand(n_PC, length_)\n",
    "                \n",
    "                # calculating entropy bound using the entire trajectory length\n",
    "                rand_traj = np.zeros((n_PC, int(bound_len)))\n",
    "                for ii in range(int(bound_len/length_)):\n",
    "                    \n",
    "                    # create trajectory for entropy bound\n",
    "                    idx = ii * length_\n",
    "                    rand_traj[:, idx:idx+length_] = f_unit\n",
    "                                \n",
    "                    # update fundamental unit \n",
    "                    f_unit = np.random.rand(n_PC, length_)\n",
    "            \n",
    "                print(rand_traj.shape)\n",
    "                \n",
    "                #######################################################################\n",
    "                print(\"CALCULATING ENTROPY BOUND\")\n",
    "                [H_bound, _] = mPE_(rand_traj, order)\n",
    "                print(\"finished\")\n",
    "                #######################################################################\n",
    "                \n",
    "                f_unit = np.random.rand(n_PC, length_)\n",
    "                for j, size in enumerate(sizes):\n",
    "                    sample_traj = np.zeros((n_PC, size))\n",
    "                    for iii in range(int(size/length_)):\n",
    "                                \n",
    "                        # create trajectory for entropy bound\n",
    "                        idx = iii * length_\n",
    "                        sample_traj[:, idx:idx+length_] = f_unit\n",
    "                                \n",
    "                        # update fundamental unit \n",
    "                        f_unit = np.random.rand(n_PC, length_)\n",
    "\n",
    "                        \n",
    "                    #######################################################################\n",
    "                    print(\"CALCULATING SAMPLE ENTROPY\")\n",
    "                    [H_sample, _] = mPE_(sample_traj, order)\n",
    "                    sample_H[trial,j] = H_sample\n",
    "                    #######################################################################\n",
    "                \n",
    "                plt.plot(sizes, sample_H[trial,:])\n",
    "                plt.axhline(y=H_bound, color=\"black\", linestyle=\"--\")\n",
    "                plt.ylabel('order ' + str(order) + ' mPE')\n",
    "                plt.xlabel('sample size')\n",
    "                plt.xscale(\"log\")\n",
    "                plt.ylim([np.min(sample_H[i,:]) - 1 , H_bound + 0.5])\n",
    "                plt.title(str(n_PC) + ' Principal Components')\n",
    "                plt.grid()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Analysis - Observation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimal time lag\n",
    "filename = '/rds/general/user/lr4617/home/4th_Year_Project/CAPTURE_rat_multidimensional/raw_data/normal/traj_1/na_auto_MI_per_lag_6.npy'\n",
    "sub_sampling = np.load(filename)\n",
    "step = 5\n",
    "shifts = step*np.arange(sub_sampling.shape[0])\n",
    "plt.scatter(shifts, sub_sampling/np.max(sub_sampling))\n",
    "\n",
    "auto_mi = 0.6*np.max(sub_sampling)\n",
    "time_lags = np.where(np.isclose(sub_sampling, auto_mi, rtol=1e-02, atol=1e-02))\n",
    "time_lag = shifts[time_lags[0][0]]\n",
    "\n",
    "# load entire high-dimensional trajectory\n",
    "path = '/rds/general/user/lr4617/home/4th_Year_Project/CAPTURE_rat_multidimensional/raw_data/normal/traj_1/trajectories_na/'\n",
    "trajectories = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing invalid values (e.g. NaN)\n",
    "# input data is already normalized (z-score) but needs to get rid of non-valued datapoints\n",
    "nan_cols = []\n",
    "for i, time_bin in enumerate(trajectories):\n",
    "    if time_bin != 'behavs':\n",
    "        trajectory = loadmat(path + time_bin)\n",
    "        trajectory = trajectory['trajectory'] \n",
    "        for i in range(trajectory.shape[1]):\n",
    "            if np.isnan(trajectory[:, i]).all():\n",
    "                nan_cols.append(i)\n",
    "\n",
    "sampling_factor = time_lag\n",
    "nan_cols = np.asarray(nan_cols)\n",
    "a = 0\n",
    "\n",
    "if nan_cols.size > 0:\n",
    "    if len(np.where(nan_cols==nan_cols[0])[0])*3 == len(nan_cols):\n",
    "        sampled_trajectories = np.zeros(((trajectory.shape[0]*int(len(trajectories)/sampling_factor)), trajectory.shape[1]-len(nan_cols)))\n",
    "        all_trajectories = np.zeros((trajectory.shape[0]*int(len(trajectories)), trajectory.shape[1]-len(nan_cols)))\n",
    "else:\n",
    "    sampled_trajectories = np.zeros(((trajectory.shape[0]*int(len(trajectories)/sampling_factor)), trajectory.shape[1]))\n",
    "    all_trajectories = np.zeros((trajectory.shape[0]*int(len(trajectories)), trajectory.shape[1]))\n",
    "\n",
    "for i, time_bin in enumerate(trajectories):\n",
    "    if time_bin != 'behavs':\n",
    "        trajectory = loadmat(path + time_bin)\n",
    "        trajectory = trajectory['trajectory'] \n",
    "        if nan_cols.size > 0:\n",
    "            trajectory = np.delete(trajectory, nan_cols, 1)\n",
    "        idx = a*trajectory.shape[0]\n",
    "        idx_2 = i*trajectory.shape[0]\n",
    "        all_trajectories[idx_2:idx_2+trajectory.shape[0], 0:sampled_trajectories.shape[1]] = trajectory\n",
    "        if i % sampling_factor == 0 and sampled_trajectories.shape[0]-idx >= trajectory.shape[0]:\n",
    "            sampled_trajectories[idx:idx+trajectory.shape[0], 0:sampled_trajectories.shape[1]] = trajectory\n",
    "            a+=1\n",
    "            \n",
    "# convert nan to number when not it is a sparse recurrence (not an entire column)\n",
    "sampled_trajectories = np.nan_to_num(sampled_trajectories)\n",
    "all_trajectories = np.nan_to_num(all_trajectories)\n",
    "\n",
    "print(sampled_trajectories.shape)\n",
    "print(all_trajectories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data according to explained variance values using linear PCA\n",
    "for trial in range(20):\n",
    "    for n_PC in range(1,4):\n",
    "        pca = PCA(n_components=n_PC)\n",
    "        reduced_traj = pca.fit_transform(sampled_trajectories)\n",
    "        reduced_traj = np.reshape(reduced_traj, (reduced_traj.shape[0], n_PC))\n",
    "\n",
    "        # sample size effect on dynamical entropy bound\n",
    "        min_portion = 20\n",
    "        orders = [3, 4]\n",
    "        min_length = int(len(reduced_traj)/min_portion)\n",
    "        sizes = np.arange(min_length, len(reduced_traj), 2*min_length)\n",
    "        sample_H = np.zeros((len(orders), sizes.size))\n",
    "\n",
    "        for i, order in enumerate(orders):\n",
    "            # calculating entropy bound using the entire trajectory length\n",
    "            if random:\n",
    "                rand_uniform_traj = np.random.rand(len(reduced_traj))\n",
    "                rand_uniform_traj = np.reshape(rand_uniform_traj, (len(reduced_traj), n_PC))\n",
    "                [H_bound, _] = mPE_(rand_uniform_traj, order)\n",
    "            else:\n",
    "                [H_bound, _] = mPE_(reduced_traj, order)\n",
    "\n",
    "            for j, size in enumerate(sizes):\n",
    "                if random:\n",
    "                    valid = False\n",
    "                    while valid==False:\n",
    "                        start_idx = np.random.randint(0, high=len(reduced_traj))\n",
    "                        if start_idx + size <= len(reduced_traj):\n",
    "                            valid = True\n",
    "                    traj = reduced_traj[start_idx:start_idx + size, :]\n",
    "                else:\n",
    "                    traj = reduced_traj[0:size, :]\n",
    "                [H_sample, _] = mPE_(traj, order)\n",
    "                sample_H[i,j] = H_sample\n",
    "\n",
    "            plt.plot(sizes, sample_H[i,:])\n",
    "            plt.axhline(y=H_bound, color=\"black\", linestyle=\"--\")\n",
    "            plt.ylabel('order ' + str(order) + ' mPE')\n",
    "            plt.xlabel('sample size')\n",
    "            plt.xscale(\"log\")\n",
    "            plt.ylim([np.min(sample_H[i,:]) - 1 , H_bound + 0.5])\n",
    "            plt.title(str(n_PC) + ' Principal Components')\n",
    "            plt.grid()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prj_env_conda]",
   "language": "python",
   "name": "conda-env-prj_env_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
