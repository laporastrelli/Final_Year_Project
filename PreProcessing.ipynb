{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import itertools as it\n",
    "import scipy.special as psi\n",
    "plt.style.use('classic')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import stats\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from scipy.integrate import quad\n",
    "from scipy.io import savemat\n",
    "from tempfile import TemporaryFile\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from mpl_toolkits import mplot3d\n",
    "from mPE_fn import mPE\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading entire high-dimensional trajectory\n",
    "path = '/rds/general/user/lr4617/home/4th_Year_Project/CAPTURE_rat_multidimensional/raw_data/normal/traj_1/trajectories/'\n",
    "trajectories = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216000, 57)\n",
      "(8100000, 57)\n"
     ]
    }
   ],
   "source": [
    "# removing invalid values (e.g. NaN)\n",
    "# input data is already normalized (z-score) but needs to get rid of non-valued datapoints\n",
    "nan_cols = []\n",
    "for i, time_bin in enumerate(trajectories):\n",
    "    trajectory = loadmat(path + time_bin)\n",
    "    trajectory = trajectory['trajectory'] \n",
    "    for i in range(trajectory.shape[1]):\n",
    "        if np.isnan(trajectory[:, i]).all():\n",
    "            nan_cols.append(i)\n",
    "\n",
    "sampling_factor = 30\n",
    "nan_cols = np.asarray(nan_cols)\n",
    "if len(np.where(nan_cols==nan_cols[0])[0])*3 == len(nan_cols):\n",
    "    sampled_trajectories = np.zeros(((trajectory.shape[0]*int(len(trajectories)/sampling_factor)), trajectory.shape[1]-3))\n",
    "    all_trajectories = np.zeros((trajectory.shape[0]*int(len(trajectories)), trajectory.shape[1]-3))\n",
    "    a = 0\n",
    "    for i, time_bin in enumerate(trajectories):\n",
    "        trajectory = loadmat(path + time_bin)\n",
    "        trajectory = trajectory['trajectory'] \n",
    "        trajectory = np.delete(trajectory, nan_cols, 1)\n",
    "        idx = a*trajectory.shape[0]\n",
    "        idx_2 = i*trajectory.shape[0]\n",
    "        all_trajectories[idx_2:idx_2+trajectory.shape[0], 0:sampled_trajectories.shape[1]] = trajectory\n",
    "        if i % sampling_factor == 0 and sampled_trajectories.shape[0]-idx >= trajectory.shape[0]:\n",
    "            sampled_trajectories[idx:idx+trajectory.shape[0], 0:sampled_trajectories.shape[1]] = trajectory\n",
    "            a+=1\n",
    "        \n",
    "print(sampled_trajectories.shape)\n",
    "print(all_trajectories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and Second Moment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([18, 38, 46, 47, 56]),)\n"
     ]
    }
   ],
   "source": [
    "# mean \n",
    "means = [np.mean(all_trajectories[:, i]) for i in range(all_trajectories.shape[1])]\n",
    "\n",
    "# standard deviation\n",
    "stds = [np.std(all_trajectories[:, i]) for i in range(all_trajectories.shape[1])]\n",
    "max_dim_std = np.where(np.isclose(stds, max(stds)))\n",
    "print(max_dim_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling Frequency (using MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sequence, decimals=0):\n",
    "    '''\n",
    "    input: \n",
    "        - 1D sequence of rv observations\n",
    "    return: \n",
    "        - probability vector\n",
    "    '''\n",
    "    \n",
    "    if sequence.shape[0] < sequence.shape[1]:\n",
    "        sequence = np.transpose(sequence)\n",
    "    \n",
    "    # round input sequence to avoid sparse probability vector\n",
    "    sequence = np.round(sequence, decimals)\n",
    "    unique = np.unique(sequence, axis=0)\n",
    "    n_triplets = len(unique)\n",
    "\n",
    "    # fill probability vector\n",
    "    prob_vector = np.zeros((n_triplets, 1))\n",
    "    for row in sequence:\n",
    "        occurrences = len(np.where(np.all(np.isclose(sequence, row), axis=1))[0])\n",
    "        idx = np.where(np.all(np.isclose(unique, row), axis=1))\n",
    "        if prob_vector[idx[0]] == 0:\n",
    "            prob_vector[idx[0]] = occurrences/(sequence.shape[0])\n",
    "    return prob_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_probability(*argv):\n",
    "    '''\n",
    "    input: \n",
    "        - sequence_1 of rv_1 observations\n",
    "        - sequence_2 of rv_2 observations\n",
    "        - number of dimensions of input sequences to consider(default = all)\n",
    "    return: \n",
    "        - joint probability vector\n",
    "    '''\n",
    "    n_args = len(argv)\n",
    "    if n_args == 2:\n",
    "        s1 = argv[0]\n",
    "        s2 = argv[1]\n",
    "        dims = s1.shape[1]\n",
    "        decimals = 0\n",
    "        \n",
    "    if n_args == 3:\n",
    "        s1 = argv[0]\n",
    "        s2 = argv[1]\n",
    "        dims = argv[2]\n",
    "        # select dims based on input\n",
    "        s1 = s1[:, 0:dims]\n",
    "        s2 = s2[:, 0:dims]\n",
    "        decimals = 0\n",
    "        \n",
    "    # checking that the dimensions of the input sequences are in the right order\n",
    "    if s1.shape[0] < s1.shape[1]:\n",
    "        s1 = np.transpose(s1)\n",
    "    if s2.shape[0] < s2.shape[1]:\n",
    "        s2 = np.transpose(s2)\n",
    "    \n",
    "    s1 = np.around(s1, decimals)\n",
    "    s2 = np.around(s2, decimals) \n",
    "    \n",
    "    # here we assume that the input sequences are already rounded (n_observations x n_dimensions)\n",
    "    unique_s1 = np.unique(s1, axis=0)\n",
    "    n_triplets_s1 = len(unique_s1)\n",
    "    unique_s2 = np.unique(s2, axis=0)\n",
    "    n_triplets_s2 = len(unique_s2)\n",
    "    \n",
    "    joint_data = np.concatenate((s1, s2), axis=1)\n",
    "    print('JOINT SPACE SIZE: ', joint_data.shape)\n",
    "    \n",
    "    # filling joint probability matrix\n",
    "    joint_prob_matrix = np.zeros((n_triplets_s1, n_triplets_s2))\n",
    "    occurrences, idx_s1, idx_s2 = 0, 0, 0\n",
    "    for joint_array in joint_data:\n",
    "        occurrences = len(np.where(np.all(np.isclose(joint_data, joint_array), axis=1))[0])\n",
    "        idx_s1 = np.where(np.all(np.isclose(unique_s1, joint_array[0:dims]), axis=1))\n",
    "        idx_s2 = np.where(np.all(np.isclose(unique_s2, joint_array[dims:2*dims]), axis=1))\n",
    "        if joint_prob_matrix[idx_s1[0][0], idx_s2[0][0]] == 0:\n",
    "            joint_prob_matrix[idx_s1[0][0], idx_s2[0][0]] = (occurrences/joint_data.shape[0])\n",
    "            \n",
    "    return joint_prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(prob_s1s2, prob_s2):\n",
    "    E_cond = 0\n",
    "    for i in range(prob_s1s2.shape[0]):\n",
    "        for j in range(prob_s1s2.shape[1]):\n",
    "            E_cond += prob_s1s2[i,j] * math.log((prob_s2[j]/prob_s1s2[i,j]), 2)\n",
    "            \n",
    "    return E_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_entropy(prob_s1s2):\n",
    "    E_joint = 0\n",
    "    for i in range(prob_s1s2.shape[0]):\n",
    "        for j in range(prob_s1s2.shape[1]):\n",
    "            if prob_s1s2[i,j] > 0:\n",
    "                E_joint += prob_s1s2[i,j] * math.log((1/prob_s1s2[i,j]), 2)\n",
    "            \n",
    "    return E_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling frequency vs MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (2700, 6)\n",
      "1.0\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[3.241357] [4.43670625] [1.70766937]\n",
      "CALCULATING PROBABILITY VECTORS...\n"
     ]
    }
   ],
   "source": [
    "fs_og = 300\n",
    "length_ = int(all_trajectories.shape[0]/10)\n",
    "sub_fs = np.arange(1,50, 5)\n",
    "names = ['HeadF', 'HeadB', 'HeadL', 'SpineF', 'SpineM',\n",
    "         'SpineL', 'Offset1', 'Offset2', 'HipL', 'HipR',\n",
    "         'ElbowL', 'ArmL', 'ShoulderL', 'ShoulderR', 'ElbowR'\n",
    "         'ArmR', 'KneeR', 'KneeL', 'ShinL', 'ShinR']\n",
    "markers = [11, 15]\n",
    "MIs = np.zeros((len(sub_fs), 1))\n",
    "for i, fs in enumerate(sub_fs):\n",
    "    # step for sampling frequency\n",
    "    step = int(300/fs);\n",
    "    \n",
    "    # construct sample trajectories for the first 2 dims ('HeadF' and 'HeadB')\n",
    "    s1 = all_trajectories[0:length_:step, markers[0]*3:(markers[0]*3)+3]\n",
    "    s2 = all_trajectories[0:length_:step, markers[1]*3:(markers[1]*3)+3]\n",
    "            \n",
    "    # probability vectors for the two sequences and joint probability\n",
    "    print('CALCULATING PROBABILITY VECTORS...')\n",
    "    prob_s1 = probability(s1)\n",
    "    prob_s2 = probability(s2)\n",
    "    print('CALCULATING JOINT PROBABILITY VECTORS...') \n",
    "    prob_s1s2 = joint_probability(s1, s2) \n",
    "    print(np.sum(prob_s1s2))\n",
    "    \n",
    "    # calculating entropy values for MI\n",
    "    print('CALCULATING ENTROPY VECTORS...')\n",
    "    E_s1 = entropy(prob_s1, base=2)\n",
    "    E_s2 = entropy(prob_s2, base=2)\n",
    "    E_s1_cond_s2 = joint_entropy(prob_s1s2) - E_s2\n",
    "    print(E_s1, E_s2, E_s1_cond_s2)\n",
    "    MI = E_s1 - E_s1_cond_s2\n",
    "    MIs[i] = MI\n",
    "    \n",
    "# plotting relation between MI and sub-sampling frequency\n",
    "plt.scatter(sub_fs, MIs)\n",
    "plt.ylabel('MI')\n",
    "plt.xlabel('Sub-sampling frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prj_env_conda]",
   "language": "python",
   "name": "conda-env-prj_env_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
