{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import itertools as it\n",
    "import scipy.special as psi\n",
    "plt.style.use('classic')\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy import stats\n",
    "from numpy.random import seed\n",
    "from numpy.random import rand\n",
    "from scipy.integrate import quad\n",
    "from scipy.io import savemat\n",
    "from tempfile import TemporaryFile\n",
    "from scipy.io import loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from mpl_toolkits import mplot3d\n",
    "from mPE_fn import mPE\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading entire high-dimensional trajectory\n",
    "path = '/rds/general/user/lr4617/home/4th_Year_Project/CAPTURE_rat_multidimensional/raw_data/normal/traj_1/trajectories/'\n",
    "trajectories = os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216000, 57)\n",
      "(8100000, 57)\n"
     ]
    }
   ],
   "source": [
    "# removing invalid values (e.g. NaN)\n",
    "# input data is already normalized (z-score) but needs to get rid of non-valued datapoints\n",
    "nan_cols = []\n",
    "for i, time_bin in enumerate(trajectories):\n",
    "    trajectory = loadmat(path + time_bin)\n",
    "    trajectory = trajectory['trajectory'] \n",
    "    for i in range(trajectory.shape[1]):\n",
    "        if np.isnan(trajectory[:, i]).all():\n",
    "            nan_cols.append(i)\n",
    "\n",
    "sampling_factor = 30\n",
    "nan_cols = np.asarray(nan_cols)\n",
    "if len(np.where(nan_cols==nan_cols[0])[0])*3 == len(nan_cols):\n",
    "    sampled_trajectories = np.zeros(((trajectory.shape[0]*int(len(trajectories)/sampling_factor)), trajectory.shape[1]-3))\n",
    "    all_trajectories = np.zeros((trajectory.shape[0]*int(len(trajectories)), trajectory.shape[1]-3))\n",
    "    a = 0\n",
    "    for i, time_bin in enumerate(trajectories):\n",
    "        trajectory = loadmat(path + time_bin)\n",
    "        trajectory = trajectory['trajectory'] \n",
    "        trajectory = np.delete(trajectory, nan_cols, 1)\n",
    "        idx = a*trajectory.shape[0]\n",
    "        idx_2 = i*trajectory.shape[0]\n",
    "        all_trajectories[idx_2:idx_2+trajectory.shape[0], 0:sampled_trajectories.shape[1]] = trajectory\n",
    "        if i % sampling_factor == 0 and sampled_trajectories.shape[0]-idx >= trajectory.shape[0]:\n",
    "            sampled_trajectories[idx:idx+trajectory.shape[0], 0:sampled_trajectories.shape[1]] = trajectory\n",
    "            a+=1\n",
    "        \n",
    "print(sampled_trajectories.shape)\n",
    "print(all_trajectories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First and Second Moment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean \n",
    "means = [np.mean(all_trajectories[:, i]) for i in range(all_trajectories.shape[1])]\n",
    "\n",
    "# standard deviation\n",
    "stds = [np.std(all_trajectories[:, i]) for i in range(all_trajectories.shape[1])]\n",
    "max_dim_std = np.where(np.isclose(stds, max(stds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling Frequency (using MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(sequence, decimals=0):\n",
    "    '''\n",
    "    input: \n",
    "        - 1D sequence of rv observations\n",
    "    return: \n",
    "        - probability vector\n",
    "    '''\n",
    "    \n",
    "    if sequence.shape[0] < sequence.shape[1]:\n",
    "        sequence = np.transpose(sequence)\n",
    "    \n",
    "    # round input sequence to avoid sparse probability vector\n",
    "    sequence = np.round(sequence, decimals)\n",
    "    unique = np.unique(sequence, axis=0)\n",
    "    n_triplets = len(unique)\n",
    "\n",
    "    # fill probability vector\n",
    "    prob_vector = np.zeros((n_triplets, 1))\n",
    "    for row in sequence:\n",
    "        occurrences = len(np.where(np.all(np.isclose(sequence, row), axis=1))[0])\n",
    "        idx = np.where(np.all(np.isclose(unique, row), axis=1))\n",
    "        if prob_vector[idx[0]] == 0:\n",
    "            prob_vector[idx[0]] = occurrences/(sequence.shape[0])\n",
    "    return prob_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_probability(*argv):\n",
    "    '''\n",
    "    input: \n",
    "        - sequence_1 of rv_1 observations\n",
    "        - sequence_2 of rv_2 observations\n",
    "        - number of dimensions of input sequences to consider(default = all)\n",
    "    return: \n",
    "        - joint probability vector\n",
    "    '''\n",
    "    n_args = len(argv)\n",
    "    if n_args == 2:\n",
    "        s1 = argv[0]\n",
    "        s2 = argv[1]\n",
    "        dims = s1.shape[1]\n",
    "        decimals = 0\n",
    "        \n",
    "    if n_args == 3:\n",
    "        s1 = argv[0]\n",
    "        s2 = argv[1]\n",
    "        dims = argv[2]\n",
    "        # select dims based on input\n",
    "        s1 = s1[:, 0:dims]\n",
    "        s2 = s2[:, 0:dims]\n",
    "        decimals = 0\n",
    "        \n",
    "    # checking that the dimensions of the input sequences are in the right order\n",
    "    if s1.shape[0] < s1.shape[1]:\n",
    "        s1 = np.transpose(s1)\n",
    "    if s2.shape[0] < s2.shape[1]:\n",
    "        s2 = np.transpose(s2)\n",
    "    \n",
    "    s1 = np.around(s1, decimals)\n",
    "    s2 = np.around(s2, decimals) \n",
    "    \n",
    "    # here we assume that the input sequences are already rounded (n_observations x n_dimensions)\n",
    "    unique_s1 = np.unique(s1, axis=0)\n",
    "    n_triplets_s1 = len(unique_s1)\n",
    "    unique_s2 = np.unique(s2, axis=0)\n",
    "    n_triplets_s2 = len(unique_s2)\n",
    "    \n",
    "    joint_data = np.concatenate((s1, s2), axis=1)\n",
    "    print('JOINT SPACE SIZE: ', joint_data.shape)\n",
    "    \n",
    "    # filling joint probability matrix\n",
    "    joint_prob_matrix = np.zeros((n_triplets_s1, n_triplets_s2))\n",
    "    occurrences, idx_s1, idx_s2 = 0, 0, 0\n",
    "    for joint_array in joint_data:\n",
    "        occurrences = len(np.where(np.all(np.isclose(joint_data, joint_array), axis=1))[0])\n",
    "        idx_s1 = np.where(np.all(np.isclose(unique_s1, joint_array[0:dims]), axis=1))\n",
    "        idx_s2 = np.where(np.all(np.isclose(unique_s2, joint_array[dims:2*dims]), axis=1))\n",
    "        if joint_prob_matrix[idx_s1[0][0], idx_s2[0][0]] == 0:\n",
    "            joint_prob_matrix[idx_s1[0][0], idx_s2[0][0]] = (occurrences/joint_data.shape[0])\n",
    "            \n",
    "    return joint_prob_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy(prob_s1s2, prob_s2):\n",
    "    E_cond = 0\n",
    "    for i in range(prob_s1s2.shape[0]):\n",
    "        for j in range(prob_s1s2.shape[1]):\n",
    "            E_cond += prob_s1s2[i,j] * math.log((prob_s2[j]/prob_s1s2[i,j]), 2)\n",
    "            \n",
    "    return E_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_entropy(prob_s1s2):\n",
    "    E_joint = 0\n",
    "    for i in range(prob_s1s2.shape[0]):\n",
    "        for j in range(prob_s1s2.shape[1]):\n",
    "            if prob_s1s2[i,j] > 0:\n",
    "                E_joint += prob_s1s2[i,j] * math.log((1/prob_s1s2[i,j]), 2)\n",
    "            \n",
    "    return E_joint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-sampling frequency vs MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (2700, 6)\n",
      "0.9999999999999999\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[4.88101572] [4.82060177] [1.58048517]\n",
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (16200, 6)\n",
      "1.0\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[4.89540654] [4.85592927] [1.68080895]\n",
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (30000, 6)\n",
      "1.0\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[4.90227779] [4.8573105] [1.70022187]\n",
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (45000, 6)\n",
      "0.9999999999999999\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[4.90326487] [4.85655709] [1.70678899]\n",
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (57858, 6)\n",
      "0.9999999999999999\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[4.90194096] [4.85717843] [1.70501746]\n",
      "CALCULATING PROBABILITY VECTORS...\n",
      "CALCULATING JOINT PROBABILITY VECTORS...\n",
      "JOINT SPACE SIZE:  (73637, 6)\n",
      "1.0\n",
      "CALCULATING ENTROPY VECTORS...\n",
      "[4.90166418] [4.85754323] [1.70545773]\n",
      "CALCULATING PROBABILITY VECTORS...\n"
     ]
    }
   ],
   "source": [
    "fs_og = 300\n",
    "length_ = int(all_trajectories.shape[0]/10)\n",
    "sub_fs = np.arange(1,50, 5)\n",
    "names = ['HeadF', 'HeadB', 'HeadL', 'SpineF', 'SpineM',\n",
    "         'SpineL', 'Offset1', 'Offset2', 'HipL', 'HipR',\n",
    "         'ElbowL', 'ArmL', 'ShoulderL', 'ShoulderR', 'ElbowR'\n",
    "         'ArmR', 'KneeR', 'KneeL', 'ShinL', 'ShinR']\n",
    "markers = [11, 15]\n",
    "MIs = np.zeros((len(sub_fs), 1))\n",
    "for i, fs in enumerate(sub_fs):\n",
    "    # step for sampling frequency\n",
    "    step = int(300/fs);\n",
    "    \n",
    "    # construct sample trajectories for the first 2 dims ('HeadF' and 'HeadB')\n",
    "    s1 = all_trajectories[0:length_:step, dims[0]*3:(dims[0]*3)+3]\n",
    "    s2 = all_trajectories[0:length_:step, dims[1]*3:(dims[1]*3)+3]\n",
    "            \n",
    "    # probability vectors for the two sequences and joint probability\n",
    "    print('CALCULATING PROBABILITY VECTORS...')\n",
    "    prob_s1 = probability(s1)\n",
    "    prob_s2 = probability(s2)\n",
    "    print('CALCULATING JOINT PROBABILITY VECTORS...') \n",
    "    prob_s1s2 = joint_probability(s1, s2) \n",
    "    print(np.sum(prob_s1s2))\n",
    "    \n",
    "    # calculating entropy values for MI\n",
    "    print('CALCULATING ENTROPY VECTORS...')\n",
    "    E_s1 = entropy(prob_s1, base=2)\n",
    "    E_s2 = entropy(prob_s2, base=2)\n",
    "    E_s1_cond_s2 = joint_entropy(prob_s1s2) - E_s2\n",
    "    print(E_s1, E_s2, E_s1_cond_s2)\n",
    "    MI = E_s1 - E_s1_cond_s2\n",
    "    MIs[i] = MI\n",
    "    \n",
    "# plotting relation between MI and sub-sampling frequency\n",
    "plt.scatter(sub_fs, MIs)\n",
    "plt.ylabel('MI')\n",
    "plt.xlabel('Sub-sampling frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting inter-dimensional variance with PCA\n",
    "pca = PCA()\n",
    "pca.fit(all_trajectories)\n",
    "\n",
    "plt.scatter(np.arange(all_trajectories.shape[1]), pca.explained_variance_ratio_)\n",
    "print(sum(pca.explained_variance_ratio_[0:3]))\n",
    "plt.ylabel('Expalined Variance Ratio')\n",
    "plt.xlabel('Dimension')\n",
    "plt.show()\n",
    "\n",
    "# reduce data according to explained variance values using linear PCA\n",
    "pca = PCA(n_components=3)\n",
    "reduced_traj = pca.fit_transform(all_trajectories)\n",
    "reduced_traj.shape\n",
    "\n",
    "print(np.cov(np.transpose(reduced_traj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = MDS(n_components=3)\n",
    "reduced_traj_MDS = embedding.fit_transform(sampled_trajectories[0:3600, :])\n",
    "reduced_traj_MDS.shape\n",
    "\n",
    "print(np.cov(np.transpose(reduced_traj_MDS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: Kernel-PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kPCA = KernelPCA(n_components=3, kernel = 'linear')\n",
    "reduced_traj_k = kPCA.fit_transform((all_trajectories))\n",
    "reduced_traj_k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_init = np.cov(np.transpose(first_trajectories))\n",
    "cov_sampled = np.cov(np.transpose(sampled_trajectories))\n",
    "vars_ = np.zeros((cov_init.shape[0], 2))\n",
    "means_ = np.zeros((first_trajectories.shape[1],1))\n",
    "for i in range(cov_init.shape[0]):\n",
    "    vars_[i, 0] = cov_init[i,i]\n",
    "    vars_[i, 1] = cov_sampled[i,i]\n",
    "    means_[i] = np.mean(first_trajectories[:, i])\n",
    "\n",
    "# update initial covariance\n",
    "n_init = first_trajectories.shape[0]\n",
    "for i, time_bin in enumerate(trajectories):\n",
    "    if i >= sampling_factor:\n",
    "        print('trajectory: ', i)\n",
    "        print(path + time_bin)\n",
    "        trajectory = loadmat(path + time_bin)\n",
    "        trajectory = trajectory['trajectory'] \n",
    "        trajectory = np.delete(trajectory, nan_cols, 1)\n",
    "        for a, row in enumerate(trajectory):\n",
    "            for j, obs in enumerate(row):\n",
    "                new_mean_j = (n_init*means_[j] + obs)/(n_init + 1)\n",
    "                for k in range(j, len(row)):\n",
    "                    cov_temp = cov_init[j,k]\n",
    "                    new_mean_k = (n_init*means_[k] + row[k])/(n_init + 1)\n",
    "                    cov_init[j,k] = (n_init/n_init + 1)*cov_temp + ((1/n_init)*(obs-new_mean_j)*(row[k]-new_mean_k))\n",
    "                    cov_init[k,j] = cov_init[j,k]\n",
    "                                                                    \n",
    "            n_init = n_init+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_average(data, window_length, shift_length):\n",
    "    av_data = []\n",
    "    a = 0\n",
    "    for i in range(0,len(data),shift_length):\n",
    "        if i>=window_length:\n",
    "            av_data.append(np.mean(data[i-window_length:i]))\n",
    "            a = a + 1\n",
    "    return av_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x_traj = all_trajectories[:, 0]\n",
    "sample_x_traj = mov_average(sample_x_traj, 9000, 1)\n",
    "\n",
    "plt.plot(np.arange(len(sample_x_traj)), sample_x_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_vector(raw_data, max_size, min_, decimals=1):\n",
    "    rounded_data = [(round(x * 10**(decimals)))/(10**decimals) for x in raw_data]\n",
    "    sorted_data = np.sort(rounded_data)\n",
    "    sorted_data = np.array(sorted_data, dtype=np.float64)\n",
    "\n",
    "    already_seen = []\n",
    "    prob_vector = np.zeros((max_size+1))\n",
    "    occurrences = 0\n",
    "    idx = 0\n",
    "    for datapoint in sorted_data:\n",
    "        if datapoint not in already_seen:\n",
    "            occurrences = np.count_nonzero(sorted_data == datapoint)\n",
    "            already_seen.append(datapoint)\n",
    "            idx = int((datapoint - min_)/(1/10**decimals))\n",
    "            prob_vector[idx] = (occurrences/len(sorted_data))\n",
    "    \n",
    "    return prob_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mPE_matrix(reduced_traj, bins_number, traj_number, orders=[3]):\n",
    "    mPE_vector = np.zeros((bins_number, traj_number, len(orders)))\n",
    "    traj_length = int((reduced_traj.shape[0]/bins_number)/traj_number)\n",
    "    for a, order in enumerate(orders):\n",
    "        for i in range(bins_number):\n",
    "            idx_1 = 0\n",
    "            idx = 0\n",
    "            for j in range(0, traj_length*traj_number, traj_length):\n",
    "                idx_1 = i*traj_number*traj_length \n",
    "                traj = reduced_traj[idx_1 + j: idx_1 + j + traj_length]\n",
    "                if traj.shape[0]>0:\n",
    "                    [HH, _]=mPE(traj, order)\n",
    "                    mPE_vector[i, idx, a] = HH\n",
    "                idx +=1\n",
    "    return mPE_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vel_matrix(trajectory, bins_number, traj_number, least_varaince_zdim=8*3):\n",
    "    f_s = 300\n",
    "    vel_matrix = np.zeros((bins_number, traj_number))\n",
    "    traj_length = int((reduced_traj.shape[0]/bins_number)/traj_number)\n",
    "    for i in range(bins_number):\n",
    "        idx = 0\n",
    "        idx_1 = 0\n",
    "        for j in range(0, traj_length*traj_number, traj_length):\n",
    "            idx_1 = i*traj_number*traj_length \n",
    "            traj = trajectory[idx_1 + j: idx_1 + j + traj_length, least_varaince_zdim:least_varaince_zdim+3]\n",
    "            vel_bin = 0\n",
    "            last_point = traj[0, :]\n",
    "            for point in traj:\n",
    "                vel_bin = vel_bin + distance.euclidean(point, last_point)\n",
    "                last_point = point\n",
    "            vel_matrix[i, idx] = vel_bin\n",
    "            idx += 1\n",
    "                \n",
    "    return vel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavioural_entropy(behav_time_series, bins_number, traj_number, mode):\n",
    "    mPE_vector = np.zeros((bins_number, traj_number, len(orders)))\n",
    "    traj_length = int((reduced_traj.shape[0]/bins_number)/traj_number)\n",
    "    for a, order in enumerate(orders):\n",
    "        for i in range(bins_number):\n",
    "            idx = 0\n",
    "            idx_1 = 0\n",
    "            for j in range(0, traj_length*traj_number, traj_length):\n",
    "                idx_1 = i*bins_number*traj_length \n",
    "                traj = reduced_traj[idx_1 + j: idx_1 + j + traj_length]\n",
    "                if mode=='PE':\n",
    "                    [HH,hh_norm]=mPE(traj, order)\n",
    "                elif mode=='Shannon':\n",
    "                    HH = entropy(traj, base=2)\n",
    "                mPE_vector[i, idx , a] = HH\n",
    "                idx += 1\n",
    "                \n",
    "    return mPE_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_prob_matrix(x, y, n_behaviours, truncation_decimal=1):\n",
    "    # Checking that the dimensions are in the right order\n",
    "    if x.shape[0] < x.shape[1]:\n",
    "        x = np.transpose(x)\n",
    "    if y.shape[0] < y.shape[1]:\n",
    "        y = np.transpose(y)\n",
    "        \n",
    "    # reducing position array to 2D [PC1, PC2] to avoid spoarse probability matrix\n",
    "    x = x[:, 0:2]\n",
    "    # rounding reduced trajectory to 2 decimal place\n",
    "    rounded_x = [(round(i * 10**(decimals)))/(10**decimals) for i in x]\n",
    "    rounded_x = np.asarray(rounded_x)\n",
    "    # finding max and min of reduced trajectory data\n",
    "    min_ = float('inf')\n",
    "    max_ = float('-inf')\n",
    "    for time_point in range(rounded_x.shape[0]):\n",
    "        for positions in range(rounded_x.shape[1]):\n",
    "            if rounded_x[time_point, positions] > max_:\n",
    "                max_ = rounded_x[time_point, positions]\n",
    "            elif rounded_x[time_point, positions] < min_:\n",
    "                min_ = rounded_x[time_point, positions]\n",
    "\n",
    "    min_ = (round(min_ * 10**(truncation_decimal)))/(10**truncation_decimal)\n",
    "    max_ = (round(max_ * 10**(truncation_decimal)))/(10**truncation_decimal)\n",
    "    # finding the maximimum number of elements in the rounded position matrix\n",
    "    max_size = int((max_ - min_)/(1/(10**truncation_decimal)))\n",
    "    # concatating joint data (x and y) --> (each row of joint data consits of [PC1, PC2, behaviour index])\n",
    "    joint_data = np.concatenate(rounded_x, y, axis=1)\n",
    "    \n",
    "    # filling joint probability matrix\n",
    "    already_seen = []\n",
    "    joint_prob_matrix = 2*np.ones((max_size, n_behaviours))\n",
    "    occurrences, idx_location, idx_behaviour = 0, 0, 0\n",
    "    for joint_array in rounded_x:\n",
    "        if joint_array not in already_seen:\n",
    "            occurrences = np.count_nonzero(sorted_data == joint_array)\n",
    "            already_seen.append(joint_array)\n",
    "            found = True\n",
    "            while found == False:\n",
    "                idx_location = np.random.randint(max_size)\n",
    "                if joint_prob_matrix[idx_location, idx_behaviour] == 2:\n",
    "                    found = False\n",
    "                    break\n",
    "            idx_behaviour = joint_array[joint_data.shape[1]]\n",
    "            joint_prob_matrix[idx_location, idx_behaviour] = (occurrences/joint_data.shape[0])\n",
    "    \n",
    "    return joint_prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy time-evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100000 75 30\n"
     ]
    }
   ],
   "source": [
    "# Entropy Calculation:\n",
    "# - Divide entire trajetcory in 'bins_number' bins\n",
    "# - Divide each bin for 'traj_number' trajectories\n",
    "# - Calculate mPE for each of these trajectories \n",
    "minutes = 6\n",
    "f_s = 300\n",
    "bin_length = f_s*60*minutes\n",
    "bins_number = int(reduced_traj.shape[0]/bin_length)\n",
    "traj_number = 30\n",
    "orders = [3, 5]\n",
    "\n",
    "print(reduced_traj.shape[0], bins_number, traj_number)\n",
    "\n",
    "mPE_vector = get_mPE_matrix(reduced_traj, bins_number, traj_number, orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# entropy distribution visualization\n",
    "\n",
    "# plotting mpE of the first bin \n",
    "mode = 'distribution'\n",
    "bin_n = 2\n",
    "order_n = 0\n",
    "fig = plt.figure()\n",
    "\n",
    "\n",
    "# plotting histograms of two different bins\n",
    "if mode == 'hist':\n",
    "    n, bins, patches = plt.hist(x = mPE_vector[bin_n, :, order_n] , bins='auto', color='green',\n",
    "                                alpha=0.7, rwidth=0.85)\n",
    "    n, bins, patches = plt.hist(x= mPE_vector[mPE_vector.shape[0]-5, :, order_n] , bins='auto', color='red',\n",
    "                                alpha=0.7, rwidth=0.85)\n",
    "\n",
    "# plotting mPE continuous distribution as a function of time \n",
    "elif mode == 'distribution':\n",
    "    means = np.zeros(((bins_number),1))\n",
    "    stds = np.zeros(((bins_number),1))\n",
    "    for bin_n in range(bins_number):\n",
    "        means[bin_n] = np.mean(mPE_vector[bin_n, :, order_n])\n",
    "        stds[bin_n] = np.std(mPE_vector[bin_n, :, order_n])\n",
    "        # ax = sns.displot(mPE_vector[bin_n, :, order_n],  kind=\"kde\")        \n",
    "        ax = sns.distplot(mPE_vector[bin_n, :, order_n], hist=False, kde=True, \n",
    "                          bins=0.1, \n",
    "                          hist_kws={'edgecolor':'black'},\n",
    "                          kde_kws={'linewidth': 4})\n",
    "        \n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('mPE')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(bins_number), means)\n",
    "plt.xlabel('time bin')\n",
    "plt.ylabel('mean mPE')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(bins_number), stds)\n",
    "plt.xlabel('time bin')\n",
    "plt.ylabel('mean mPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ = float('inf')\n",
    "max_ = float('-inf')\n",
    "for order in range(mPE_vector.shape[2]):\n",
    "    for bin_ in range(mPE_vector.shape[0]):\n",
    "        for traj in range(mPE_vector.shape[1]):\n",
    "            if mPE_vector[bin_, traj, order] > max_:\n",
    "                max_ = mPE_vector[bin_, traj, order]\n",
    "            elif mPE_vector[bin_, traj, order] < min_:\n",
    "                min_ = mPE_vector[bin_, traj, order]\n",
    "\n",
    "truncation_decimal = 1\n",
    "min_ = (round(min_ * 10**(truncation_decimal)))/(10**truncation_decimal)\n",
    "max_ = (round(max_ * 10**(truncation_decimal)))/(10**truncation_decimal)\n",
    "max_size = int((max_ - min_)/(1/(10**truncation_decimal)))\n",
    "\n",
    "# plotting J-S divergence as a function of bin number\n",
    "compare_to = prob_vector(mPE_vector[0, :, 0], max_size, min_)\n",
    "js_vector = np.zeros((mPE_vector.shape[0]))\n",
    "ks_vector = np.zeros((mPE_vector.shape[0], 2))\n",
    "ks_and_div = np.zeros((mPE_vector.shape[0], 2))\n",
    "significance_lev = 0.01\n",
    "for i in range(mPE_vector.shape[0]):\n",
    "    js_vector[i] = distance.jensenshannon(prob_vector(mPE_vector[i, :, 0], max_size, min_), compare_to)\n",
    "    ks_and_div[i, 0] = js_vector[i]\n",
    "    if i > 0:\n",
    "        ks_vector[i, :] = stats.ks_2samp(mPE_vector[i, :, 0], mPE_vector[0, :, 0])\n",
    "        if ks_vector[i, 1] <= significance_lev:\n",
    "            ks_and_div[i, 1] = 1\n",
    "        else:\n",
    "            ks_and_div[i, 1] = 0\n",
    "            \n",
    "fig = plt.figure()\n",
    "# plotting velocity profile of mouse (point-wise)\n",
    "plt.plot(np.arange(mPE_vector.shape[0]), js_vector)\n",
    "plt.xlabel('bin number')\n",
    "plt.ylabel('J-S Divergence')\n",
    "\n",
    "js_rejected = np.where(ks_and_div[:,1] == 1)\n",
    "js_th = float('inf')\n",
    "for i in js_rejected[0]:\n",
    "    if ks_and_div[i, 0] < js_th:\n",
    "        js_th = ks_and_div[i, 0]\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.scatter(ks_and_div[:, 0], ks_and_div[:, 1])\n",
    "plt.axhline(y=significance_lev, xmin=0, xmax=1)\n",
    "plt.axvline(x=js_th, ymin=0, ymax=1.2)\n",
    "plt.xlabel('J-S Divergence')\n",
    "plt.ylabel('P-Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity vs Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate instantaneous velocity of each datapoint (maybe only consider x-y dims)\n",
    "lest_varaince_dim = 8*3\n",
    "minutes = 5\n",
    "fs = 300\n",
    "bin_length = fs*60*minutes\n",
    "\n",
    "bins_number = int(reduced_traj.shape[0]/bin_length)\n",
    "traj_number = 30\n",
    "orders = [5]\n",
    "vel_matrix = get_vel_matrix(all_trajectories, bins_number, traj_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting bin-velocity against corresponding mPE\n",
    "print(bins_number)\n",
    "print(mPE_vector.shape)\n",
    "vel_vector = vel_matrix.flatten()\n",
    "mPE_vector_ = mPE_vector.flatten()\n",
    "p = np.polyfit(mPE_vector_, vel_vector, 2)\n",
    "x_new = np.linspace(4,12,200)\n",
    "ffit = np.polyval(p, x_new)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(mPE_vector_, vel_vector)\n",
    "plt.xlabel('mPE')\n",
    "plt.ylabel('velocity')\n",
    "plt.plot(x_new, ffit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prj_env_conda]",
   "language": "python",
   "name": "conda-env-prj_env_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
